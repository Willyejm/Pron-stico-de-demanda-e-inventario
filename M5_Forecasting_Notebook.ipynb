{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1217ff",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Caso de Uso ML en Cadena de Suministro: Pron√≥stico de Demanda (M5, Walmart)\n",
    "**Notebook base listo para correr** ‚Äî Incluye: descarga de datos (opcional v√≠a Kaggle), EDA m√≠nima, *feature engineering*, validaci√≥n temporal con ventana rodante, entrenamiento con **LightGBM**, m√©tricas (**MAE, RMSE, WMAPE**), e inferencia de un horizonte de 28 d√≠as.\n",
    "\n",
    "> **Requisitos (elige tu ruta):**\n",
    "> 1) **Colab + Kaggle API** (recomendado): sube tu `kaggle.json` y corre la celda de descarga.  \n",
    "> 2) **Local (descarga manual)**: descarga el dataset **M5 Forecasting - Accuracy** desde Kaggle y descomprime los CSV en una carpeta local.  \n",
    ">\n",
    "> **Archivos esperados (`calendar.csv`, `sales_train_validation.csv`, `sell_prices.csv`)** del dataset **M5 Forecasting - Accuracy**.\n",
    ">\n",
    "> Para acelerar en equipos modestos, puedes limitar el n√∫mero de SKU/tienda con `N_ITEMS_SAMPLE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Instalar dependencias (ejecuta si usas Colab o un entorno limpio)\n",
    "# Si est√°s en local y ya tienes las librer√≠as, puedes omitir.\n",
    "# Nota: En Colab se recomienda reiniciar el runtime tras instalar.\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as e:\n",
    "    !pip -q install pandas numpy lightgbm scikit-learn matplotlib pyarrow python-dateutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Imports y configuraci√≥n global\n",
    "import os, gc, warnings, math, json, pathlib\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Versiones -> pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34df98d",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Configuraci√≥n\n",
    "- `DATA_DIR`: carpeta donde est√°n los CSV del M5 (`calendar.csv`, `sales_train_validation.csv`, `sell_prices.csv`).\n",
    "- `USE_KAGGLE_DOWNLOAD`: si `True`, descargar√° desde Kaggle (requiere `kaggle.json`).\n",
    "- `N_ITEMS_SAMPLE`: n√∫mero de SKU a muestrear para pruebas r√°pidas (None = usar todos).  \n",
    "- `STATE_FILTER` / `STORE_FILTER`: filtra por estado o tienda (√∫til para acelerar).  \n",
    "- `HORIZON`: horizonte de predicci√≥n (M5 = 28 d√≠as).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Par√°metros de ejecuci√≥n\n",
    "DATA_DIR = \"./m5_data\"        # Cambia a tu ruta local si usas descarga manual\n",
    "USE_KAGGLE_DOWNLOAD = False   # Pon True si quieres descargar con Kaggle API\n",
    "KAGGLE_DATASET = \"m5-forecasting-accuracy\"  # M5 (Accuracy)\n",
    "\n",
    "# Submuestreo para acelerar en pruebas (None usa todo)\n",
    "N_ITEMS_SAMPLE = 500          # e.g., 500 SKU; pon None para todos\n",
    "STATE_FILTER   = None         # e.g., \"CA\"\n",
    "STORE_FILTER   = None         # e.g., \"CA_1\"\n",
    "\n",
    "HORIZON = 28                  # Horizonte oficial M5\n",
    "N_FOLDS = 2                   # N¬∫ de cortes de validaci√≥n rodante\n",
    "MAX_ESTIMATORS = 1500         # √Årboles LGBM\n",
    "EARLY_STOP = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title (Opcional) Descarga Kaggle - Requiere subir tu kaggle.json\n",
    "# Ejecuta SOLO si USE_KAGGLE_DOWNLOAD = True\n",
    "if USE_KAGGLE_DOWNLOAD:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    # Instalar kaggle si es necesario\n",
    "    try:\n",
    "        import kaggle\n",
    "    except:\n",
    "        !pip -q install kaggle\n",
    "\n",
    "    # Configurar credenciales (sube tu kaggle.json a la carpeta actual)\n",
    "    # En Colab: arrastra kaggle.json o ejecuta desde drive.\n",
    "    KAGGLE_JSON = \"kaggle.json\"\n",
    "    if not os.path.exists(KAGGLE_JSON):\n",
    "        print(\"‚ö†Ô∏è Sube tu kaggle.json (desde tu cuenta de Kaggle) al directorio actual y reintenta.\")\n",
    "    else:\n",
    "        os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "        import shutil, stat\n",
    "        shutil.copy(KAGGLE_JSON, os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
    "        os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), stat.S_IRUSR | stat.S_IWUSR)\n",
    "        # Descargar y descomprimir\n",
    "        !kaggle competitions download -c {KAGGLE_DATASET} -p {DATA_DIR}\n",
    "        # Archivos a extraer: calendar.csv, sales_train_validation.csv, sell_prices.csv\n",
    "        import zipfile, glob\n",
    "        for z in glob.glob(os.path.join(DATA_DIR, \"*.zip\")):\n",
    "            print(\"Descomprimiendo:\", z)\n",
    "            with zipfile.ZipFile(z, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(DATA_DIR)\n",
    "        print(\"‚úÖ Descarga y extracci√≥n completadas en:\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9cd83",
   "metadata": {},
   "source": [
    "\n",
    "## üì• Carga de datos (M5)\n",
    "Se utilizan los 3 CSV principales:\n",
    "- `calendar.csv` (calendario, eventos, SNAP), \n",
    "- `sales_train_validation.csv` (ventas por SKU-tienda en formato ancho `d_1..d_n`),\n",
    "- `sell_prices.csv` (precios por semana/SKU/tienda).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaae63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Cargar CSV del M5\n",
    "def memory_usage(df):\n",
    "    return f\"{df.memory_usage(deep=True).sum()/1024**2:,.2f} MB\"\n",
    "\n",
    "def load_m5_data(data_dir):\n",
    "    cal_types = {\n",
    "        \"event_name_1\":\"category\", \"event_type_1\":\"category\",\n",
    "        \"event_name_2\":\"category\", \"event_type_2\":\"category\",\n",
    "        \"weekday\":\"category\", \"wm_yr_wk\":\"int32\",\n",
    "        \"wday\":\"int8\", \"month\":\"int8\", \"year\":\"int16\", \"snap_CA\":\"int8\", \"snap_TX\":\"int8\", \"snap_WI\":\"int8\"\n",
    "    }\n",
    "    calendar = pd.read_csv(os.path.join(data_dir, \"calendar.csv\"), dtype=cal_types, parse_dates=[\"date\"])\n",
    "\n",
    "    price_types = {\"store_id\":\"category\", \"item_id\":\"category\", \"wm_yr_wk\":\"int32\", \"sell_price\":\"float32\"}\n",
    "    sell_prices = pd.read_csv(os.path.join(data_dir, \"sell_prices.csv\"), dtype=price_types)\n",
    "\n",
    "    # Ventas: formato ancho (d_1..d_n). Reducimos dtypes categ√≥ricos\n",
    "    sale_types = {\n",
    "        \"item_id\":\"category\",\"dept_id\":\"category\",\"cat_id\":\"category\",\"store_id\":\"category\",\"state_id\":\"category\"\n",
    "    }\n",
    "    sales = pd.read_csv(os.path.join(data_dir, \"sales_train_validation.csv\"), dtype=sale_types)\n",
    "    return calendar, sell_prices, sales\n",
    "\n",
    "assert os.path.exists(DATA_DIR), f\"No se encontr√≥ DATA_DIR={DATA_DIR}. Aj√∫stalo o usa Kaggle download.\"\n",
    "calendar, sell_prices, sales = load_m5_data(DATA_DIR)\n",
    "\n",
    "print(\"calendar:\", calendar.shape, \"|\", memory_usage(calendar))\n",
    "print(\"sell_prices:\", sell_prices.shape, \"|\", memory_usage(sell_prices))\n",
    "print(\"sales:\", sales.shape, \"|\", memory_usage(sales))\n",
    "\n",
    "display(calendar.head(2))\n",
    "display(sell_prices.head(2))\n",
    "display(sales.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a1957",
   "metadata": {},
   "source": [
    "\n",
    "## üîé Filtro y submuestreo (opcional para acelerar)\n",
    "Puedes filtrar por `STATE_FILTER`/`STORE_FILTER`, o muestrear `N_ITEMS_SAMPLE` SKU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Aplicar filtros / submuestreo\n",
    "def filter_and_sample_sales(df, state=None, store=None, n_items_sample=None, seed=42):\n",
    "    if state is not None:\n",
    "        df = df[df[\"state_id\"]==state]\n",
    "    if store is not None:\n",
    "        df = df[df[\"store_id\"]==store]\n",
    "    if n_items_sample is not None:\n",
    "        # muestrear por item_id dentro del filtro\n",
    "        items = df[\"item_id\"].unique()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        take = n_items_sample if n_items_sample < len(items) else len(items)\n",
    "        sample_items = rng.choice(items, size=take, replace=False)\n",
    "        df = df[df[\"item_id\"].isin(sample_items)]\n",
    "    return df\n",
    "\n",
    "sales_filtered = filter_and_sample_sales(\n",
    "    sales.copy(), state=STATE_FILTER, store=STORE_FILTER, n_items_sample=N_ITEMS_SAMPLE, seed=SEED\n",
    ")\n",
    "print(\"Sales filtrado:\", sales_filtered.shape, \"|\", memory_usage(sales_filtered))\n",
    "display(sales_filtered.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca30f9",
   "metadata": {},
   "source": [
    "\n",
    "## üîÅ Transformaci√≥n wide ‚Üí long y *join* con calendario y precios\n",
    "- Pasamos de columnas `d_1..d_n` a filas por d√≠a.  \n",
    "- Unimos con `calendar` por `d`/`date`.  \n",
    "- Unimos con `sell_prices` por `store_id, item_id, wm_yr_wk`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Wide ‚Üí Long y uniones\n",
    "# Identificamos columnas d_*\n",
    "d_cols = [c for c in sales_filtered.columns if c.startswith(\"d_\")]\n",
    "id_cols = [c for c in sales_filtered.columns if c not in d_cols]\n",
    "\n",
    "long_df = sales_filtered.melt(id_vars=id_cols, value_vars=d_cols, var_name=\"d\", value_name=\"sales\")\n",
    "long_df[\"sales\"] = long_df[\"sales\"].astype(\"float32\")\n",
    "\n",
    "# Unir con calendario (obtener fecha y wm_yr_wk)\n",
    "cal_small = calendar[[\"d\",\"date\",\"wm_yr_wk\",\"wday\",\"month\",\"year\",\"snap_CA\",\"snap_TX\",\"snap_WI\",\"event_name_1\",\"event_type_1\"]].copy()\n",
    "long_df = long_df.merge(cal_small, on=\"d\", how=\"left\")\n",
    "\n",
    "# Unir con precios\n",
    "long_df = long_df.merge(sell_prices, on=[\"store_id\",\"item_id\",\"wm_yr_wk\"], how=\"left\")\n",
    "\n",
    "# Rellenar precios faltantes con √∫ltimo valor conocido por item_id/store\n",
    "long_df[\"sell_price\"] = long_df.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].transform(lambda s: s.ffill().bfill())\n",
    "long_df[\"sell_price\"] = long_df[\"sell_price\"].astype(\"float32\")\n",
    "\n",
    "print(\"long_df:\", long_df.shape, \"|\", memory_usage(long_df))\n",
    "display(long_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309af2d",
   "metadata": {},
   "source": [
    "\n",
    "## üß© *Feature engineering* clave\n",
    "- **Lags**: `lag_7`, `lag_28`  \n",
    "- **Rolling windows**: `rmean_7`, `rmean_28`, `rstd_7`, `rstd_28`  \n",
    "- **Precio**: `sell_price`, `price_change_rate`, `price_rel_rolling_28`  \n",
    "- **Calendario**: `wday`, `month`, `year`, `snap_state`, `event_type_1` (codificado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94838dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Crear features (lags, rolling, precio, calendario)\n",
    "def add_calendar_features(df):\n",
    "    df[\"wday\"] = df[\"wday\"].astype(\"int8\")\n",
    "    df[\"month\"] = df[\"month\"].astype(\"int8\")\n",
    "    df[\"year\"] = df[\"year\"].astype(\"int16\")\n",
    "    # SNAP seg√∫n estado de la tienda\n",
    "    df[\"snap\"] = 0\n",
    "    df.loc[df[\"state_id\"]==\"CA\",\"snap\"] = df.loc[df[\"state_id\"]==\"CA\",\"snap_CA\"]\n",
    "    df.loc[df[\"state_id\"]==\"TX\",\"snap\"] = df.loc[df[\"state_id\"]==\"TX\",\"snap_TX\"]\n",
    "    df.loc[df[\"state_id\"]==\"WI\",\"snap\"] = df.loc[df[\"state_id\"]==\"WI\",\"snap_WI\"]\n",
    "    df[\"snap\"] = df[\"snap\"].fillna(0).astype(\"int8\")\n",
    "    # Eventos (codificaci√≥n simple)\n",
    "    df[\"event_type_1\"] = df[\"event_type_1\"].astype(\"category\").cat.codes.astype(\"int16\")\n",
    "    return df\n",
    "\n",
    "def add_lag_roll_features(df, group_cols=[\"item_id\",\"store_id\"], sales_col=\"sales\"):\n",
    "    df = df.sort_values(group_cols + [\"date\"])\n",
    "    # Lags\n",
    "    for L in [7, 28]:\n",
    "        df[f\"lag_{L}\"] = df.groupby(group_cols)[sales_col].shift(L)\n",
    "    # Rolling means/std\n",
    "    df[\"rmean_7\"]  = df.groupby(group_cols)[\"sales\"].transform(lambda s: s.shift(1).rolling(7).mean())\n",
    "    df[\"rmean_28\"] = df.groupby(group_cols)[\"sales\"].transform(lambda s: s.shift(1).rolling(28).mean())\n",
    "    df[\"rstd_7\"]   = df.groupby(group_cols)[\"sales\"].transform(lambda s: s.shift(1).rolling(7).std())\n",
    "    df[\"rstd_28\"]  = df.groupby(group_cols)[\"sales\"].transform(lambda s: s.shift(1).rolling(28).std())\n",
    "    # Precio\n",
    "    df[\"price_change_rate\"] = df.groupby(group_cols)[\"sell_price\"].pct_change().replace([np.inf,-np.inf],0).fillna(0).astype(\"float32\")\n",
    "    df[\"price_roll28\"] = df.groupby(group_cols)[\"sell_price\"].transform(lambda s: s.rolling(28, min_periods=1).mean())\n",
    "    df[\"price_rel_rolling_28\"] = (df[\"sell_price\"] / df[\"price_roll28\"]).astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "long_df = add_calendar_features(long_df)\n",
    "long_df = add_lag_roll_features(long_df)\n",
    "\n",
    "# Eliminar filas con NaN generados por lags/rolling (calentamiento)\n",
    "min_date = long_df[\"date\"].min()\n",
    "cutoff = long_df[\"date\"].min() + pd.Timedelta(days=60)\n",
    "long_df = long_df[long_df[\"date\"]>=cutoff].copy()\n",
    "\n",
    "print(\"long_df (con features):\", long_df.shape, \"|\", memory_usage(long_df))\n",
    "display(long_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc4109",
   "metadata": {},
   "source": [
    "\n",
    "## ‚è≥ Validaci√≥n temporal (ventana rodante)\n",
    "Creamos cortes de entrenamiento/validaci√≥n manteniendo integridad temporal. Cada *fold* valida **HORIZON** d√≠as al final de la ventana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Crear cortes de backtesting\n",
    "def make_time_folds(df, date_col=\"date\", horizon=28, n_folds=2):\n",
    "    last_date = df[date_col].max()\n",
    "    folds = []\n",
    "    for i in range(n_folds):\n",
    "        valid_end = last_date - pd.Timedelta(days=horizon*i)\n",
    "        valid_start = valid_end - pd.Timedelta(days=horizon-1)\n",
    "        train_end = valid_start - pd.Timedelta(days=1)\n",
    "        # usamos todo lo anterior para entrenar\n",
    "        folds.append({\n",
    "            \"train_end\": train_end,\n",
    "            \"valid_start\": valid_start,\n",
    "            \"valid_end\": valid_end\n",
    "        })\n",
    "    folds = folds[::-1]  # del m√°s antiguo al m√°s reciente\n",
    "    return folds\n",
    "\n",
    "folds = make_time_folds(long_df, \"date\", horizon=HORIZON, n_folds=N_FOLDS)\n",
    "folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920385f",
   "metadata": {},
   "source": [
    "\n",
    "## üå≤ Modelo: LightGBM (global por SKU‚Äëtienda)\n",
    "Entrenamos un √∫nico modelo con *features* comunes para todas las series (aprendizaje global).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Entrenar y evaluar con backtesting\n",
    "import lightgbm as lgb\n",
    "\n",
    "FEATURES = [\n",
    "    \"wday\",\"month\",\"year\",\"snap\",\"event_type_1\",\n",
    "    \"sell_price\",\"price_change_rate\",\"price_rel_rolling_28\",\n",
    "    \"lag_7\",\"lag_28\",\"rmean_7\",\"rmean_28\",\"rstd_7\",\"rstd_28\"\n",
    "]\n",
    "TARGET = \"sales\"\n",
    "\n",
    "def wmape(y_true, y_pred):\n",
    "    denom = np.abs(y_true).sum()\n",
    "    return np.inf if denom == 0 else (np.abs(y_true - y_pred).sum() / denom)\n",
    "\n",
    "metrics_cv = []\n",
    "models = []\n",
    "feature_importances = []\n",
    "\n",
    "for k, f in enumerate(folds, 1):\n",
    "    train_mask = (long_df[\"date\"] <= f[\"train_end\"])\n",
    "    valid_mask = (long_df[\"date\"] >= f[\"valid_start\"]) & (long_df[\"date\"] <= f[\"valid_end\"])\n",
    "\n",
    "    train_df = long_df.loc[train_mask, FEATURES + [TARGET]].dropna()\n",
    "    valid_df = long_df.loc[valid_mask, FEATURES + [TARGET]].dropna()\n",
    "\n",
    "    X_tr, y_tr = train_df[FEATURES], train_df[TARGET]\n",
    "    X_va, y_va = valid_df[FEATURES], valid_df[TARGET]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=MAX_ESTIMATORS,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"l2\",\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_hat = model.predict(X_va)\n",
    "    mae  = mean_absolute_error(y_va, y_hat)\n",
    "    rmse = mean_squared_error(y_va, y_hat, squared=False)\n",
    "    wmp  = wmape(y_va.values, y_hat)\n",
    "    metrics_cv.append({\"fold\": k, \"MAE\": mae, \"RMSE\": rmse, \"WMAPE\": wmp})\n",
    "\n",
    "    # Importancias\n",
    "    fi = pd.Series(model.feature_importances_, index=FEATURES, name=f\"fold_{k}\")\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "    models.append(model)\n",
    "    print(f\"[Fold {k}] MAE={mae:,.3f} | RMSE={rmse:,.3f} | WMAPE={wmp:,.3%}  (Train end: {f['train_end'].date()}, Valid: {f['valid_start'].date()}‚Üí{f['valid_end'].date()})\")\n",
    "\n",
    "metrics_cv = pd.DataFrame(metrics_cv)\n",
    "display(metrics_cv)\n",
    "print(\"Promedios CV:\", metrics_cv.mean().to_dict())\n",
    "\n",
    "# Importancia promedio\n",
    "fi_df = pd.concat(feature_importances, axis=1).mean(axis=1).sort_values(ascending=False).to_frame(\"importance\")\n",
    "display(fi_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4774c",
   "metadata": {},
   "source": [
    "\n",
    "## üìà Visualizar predicci√≥n vs. real (ejemplo por un SKU‚Äëtienda)\n",
    "Seleccionamos un par `item_id`/`store_id` con datos en la √∫ltima validaci√≥n y graficamos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Gr√°fica de ejemplo\n",
    "last_fold = folds[-1]\n",
    "valid_mask = (long_df[\"date\"] >= last_fold[\"valid_start\"]) & (long_df[\"date\"] <= last_fold[\"valid_end\"])\n",
    "valid_rows = long_df.loc[valid_mask].dropna(subset=FEATURES + [TARGET]).copy()\n",
    "\n",
    "# Tomar un par SKU-tienda con variaci√≥n\n",
    "grp_cols = [\"item_id\",\"store_id\"]\n",
    "pair = (valid_rows.groupby(grp_cols)[\"sales\"].sum().sort_values(ascending=False).index[0])\n",
    "pair_df = valid_rows[(valid_rows[\"item_id\"]==pair[0]) & (valid_rows[\"store_id\"]==pair[1])].copy()\n",
    "\n",
    "X_pair = pair_df[FEATURES]\n",
    "y_pair = pair_df[TARGET].values\n",
    "\n",
    "# Usamos el √∫ltimo modelo entrenado\n",
    "y_hat_pair = models[-1].predict(X_pair)\n",
    "\n",
    "plot_df = pair_df[[\"date\"]].copy()\n",
    "plot_df[\"y_true\"] = y_pair\n",
    "plot_df[\"y_hat\"] = y_hat_pair\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"y_true\"], label=\"Real\")\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"y_hat\"], label=\"Pron√≥stico\")\n",
    "plt.title(f\"SKU: {pair[0]} | Tienda: {pair[1]}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2f15b",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Entrenamiento final y pron√≥stico futuro (28 d√≠as)\n",
    "Entrenamos con todo el hist√≥rico disponible y proyectamos **HORIZON** d√≠as hacia adelante, actualizando *lags* de forma iterativa.  \n",
    "> Nota: Para caracter√≠sticas futuras (eventos/SNAP) usamos el `calendar.csv` (ya incluye fechas posteriores a las √∫ltimas ventas). Para precios, mantenemos el √∫ltimo valor conocido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Pron√≥stico futuro\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    n_estimators=MAX_ESTIMATORS,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "train_full = long_df.dropna(subset=FEATURES + [TARGET])\n",
    "X_full, y_full = train_full[FEATURES], train_full[TARGET]\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# √öltima fecha observada\n",
    "last_date = long_df[\"date\"].max()\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=HORIZON, freq=\"D\")\n",
    "\n",
    "# Base para futuro: replicamos claves y unimos con calendario\n",
    "keys = long_df[[\"item_id\",\"store_id\",\"state_id\",\"dept_id\",\"cat_id\"]].drop_duplicates()\n",
    "cal_fut = calendar[calendar[\"date\"].isin(future_dates)][[\"date\",\"wm_yr_wk\",\"wday\",\"month\",\"year\",\"snap_CA\",\"snap_TX\",\"snap_WI\",\"event_type_1\"]]\n",
    "future_df = (\n",
    "    keys.assign(key=1)\n",
    "    .merge(cal_fut.assign(key=1), on=\"key\", how=\"left\")\n",
    "    .drop(columns=[\"key\"])\n",
    "    .merge(sell_prices, on=[\"store_id\",\"item_id\",\"wm_yr_wk\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Rellenar precios con √∫ltimo valor hist√≥rico por SKU-tienda\n",
    "last_prices = long_df.groupby([\"store_id\",\"item_id\"])[\"sell_price\"].last().rename(\"last_price\").reset_index()\n",
    "future_df = future_df.merge(last_prices, on=[\"store_id\",\"item_id\"], how=\"left\")\n",
    "future_df[\"sell_price\"] = future_df[\"sell_price\"].fillna(future_df[\"last_price\"]).astype(\"float32\")\n",
    "future_df.drop(columns=[\"last_price\"], inplace=True)\n",
    "\n",
    "# A√±adir calendario, y preparar columnas necesarias\n",
    "future_df = add_calendar_features(future_df)\n",
    "\n",
    "# Construiremos lags/rolling de forma iterativa usando una copia con hist√≥rico + futuro\n",
    "hist_cols = [\"date\",\"item_id\",\"store_id\",\"sales\",\"sell_price\",\"wday\",\"month\",\"year\",\"snap\",\"event_type_1\"]\n",
    "needed_cols = hist_cols + [\"lag_7\",\"lag_28\",\"rmean_7\",\"rmean_28\",\"rstd_7\",\"rstd_28\",\"price_change_rate\",\"price_roll28\",\"price_rel_rolling_28\"]\n",
    "\n",
    "# Tomamos hist√≥rico m√≠nimo para poder calcular lags/rolling\n",
    "hist = long_df[[\"date\",\"item_id\",\"store_id\",\"sales\",\"sell_price\",\"wday\",\"month\",\"year\",\"snap\",\"event_type_1\"]].copy()\n",
    "comb = pd.concat([hist, future_df[[\"date\",\"item_id\",\"store_id\",\"sell_price\",\"wday\",\"month\",\"year\",\"snap\",\"event_type_1\"]].assign(sales=np.nan)], ignore_index=True)\n",
    "comb = comb.sort_values([\"item_id\",\"store_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Funciones auxiliares para lags/rolling en loop\n",
    "def update_lags_roll(df):\n",
    "    df = df.sort_values([\"item_id\",\"store_id\",\"date\"])\n",
    "    for L in [7,28]:\n",
    "        df[f\"lag_{L}\"] = df.groupby([\"item_id\",\"store_id\"])[\"sales\"].shift(L)\n",
    "    df[\"rmean_7\"]  = df.groupby([\"item_id\",\"store_id\"])[\"sales\"].transform(lambda s: s.shift(1).rolling(7).mean())\n",
    "    df[\"rmean_28\"] = df.groupby([\"item_id\",\"store_id\"])[\"sales\"].transform(lambda s: s.shift(1).rolling(28).mean())\n",
    "    df[\"rstd_7\"]   = df.groupby([\"item_id\",\"store_id\"])[\"sales\"].transform(lambda s: s.shift(1).rolling(7).std())\n",
    "    df[\"rstd_28\"]  = df.groupby([\"item_id\",\"store_id\"])[\"sales\"].transform(lambda s: s.shift(1).rolling(28).std())\n",
    "    df[\"price_change_rate\"] = df.groupby([\"item_id\",\"store_id\"])[\"sell_price\"].pct_change().replace([np.inf,-np.inf],0).fillna(0).astype(\"float32\")\n",
    "    df[\"price_roll28\"] = df.groupby([\"item_id\",\"store_id\"])[\"sell_price\"].transform(lambda s: s.rolling(28, min_periods=1).mean())\n",
    "    df[\"price_rel_rolling_28\"] = (df[\"sell_price\"] / df[\"price_roll28\"]).astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "comb = update_lags_roll(comb)\n",
    "\n",
    "# Pron√≥stico iterativo d√≠a a d√≠a\n",
    "preds = []\n",
    "for d in future_dates:\n",
    "    mask = comb[\"date\"]==d\n",
    "    X = comb.loc[mask, FEATURES].copy()\n",
    "    yhat = final_model.predict(X)\n",
    "    comb.loc[mask, \"sales\"] = yhat\n",
    "    preds.append(pd.DataFrame({\"date\": d, \"item_id\": comb.loc[mask,\"item_id\"].values,\n",
    "                               \"store_id\": comb.loc[mask,\"store_id\"].values, \"yhat\": yhat}))\n",
    "    # Actualizamos lags/rolling para el siguiente d√≠a\n",
    "    comb = update_lags_roll(comb)\n",
    "\n",
    "forecast_df = pd.concat(preds, ignore_index=True)\n",
    "print(\"Pron√≥stico futuro:\", forecast_df.shape)\n",
    "display(forecast_df.head())\n",
    "\n",
    "# Guardar resultados\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "forecast_path = \"outputs/forecast_m5_lgbm.csv\"\n",
    "metrics_path  = \"outputs/cv_metrics.csv\"\n",
    "fi_path       = \"outputs/feature_importance.csv\"\n",
    "\n",
    "forecast_df.to_csv(forecast_path, index=False)\n",
    "metrics_cv.to_csv(metrics_path, index=False)\n",
    "fi_df.to_csv(fi_path)\n",
    "\n",
    "print(\"Archivos guardados:\")\n",
    "print(\"-\", forecast_path)\n",
    "print(\"-\", metrics_path)\n",
    "print(\"-\", fi_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e7a2b",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Visual r√°pido del pron√≥stico agregado (ejemplo)\n",
    "Graficamos demanda total pronosticada agregada por fecha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @title Gr√°fico del total pronosticado\n",
    "agg_forecast = forecast_df.groupby(\"date\")[\"yhat\"].sum().reset_index()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(agg_forecast[\"date\"], agg_forecast[\"yhat\"])\n",
    "plt.title(\"Pron√≥stico total (suma de yhat)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b385e87",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Siguientes pasos sugeridos\n",
    "- Ajustar hiperpar√°metros (grid/bayes) y probar **quantile regression** (P50/P90) con LightGBM para intervalos.  \n",
    "- Incorporar caracter√≠sticas adicionales: festividades espec√≠ficas, *fourier terms*, elasticidades por precio, *hierarchical reconciliation*.  \n",
    "- Conectar este pron√≥stico a una **simulaci√≥n de inventario** para calcular **stock de seguridad**, **fill-rate** y **costo total**.\n",
    "\n",
    "**Autor:** Generado por ChatGPT ¬∑ Semilla `SEED=42`  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
